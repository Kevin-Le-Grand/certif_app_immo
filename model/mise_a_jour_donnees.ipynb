{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"background-color:blue; color:white; padding: 5px;\">Notebook à executer lorsqu'il y a de nouvelles données disponible sur data.gouv.fr<br></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions2 import *\n",
    "\n",
    "# Librairies pour la décompression de fichiers et système\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import sys\n",
    "\n",
    "# Librairies pour le scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Visualisation de données\n",
    "import pandas as pd\n",
    "\n",
    "# datas processing\n",
    "import datas_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"background-color:grey; color:white; padding: 5px;\">Récupération des données disponible et insertion dans la table<br></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des codes communes , noms communes et code départements disponibles sur l'api \n",
    "url = \"https://geo.api.gouv.fr/communes\"\n",
    "response = requests.get(url)\n",
    "data_communes = response.json()\n",
    "df_communes = pd.DataFrame(data_communes)\n",
    "df_communes = df_communes.loc[:,[\"code\",\"nom\",\"codeDepartement\"]]\n",
    "df_communes = df_communes.drop_duplicates()\n",
    "df_communes.columns= [\"ID_COMMUNE\",\"NAME_COMMUNE\",\"ID_DEPT\"]\n",
    "liste_ID_COMMUNES = df_communes.ID_COMMUNE.to_list()\n",
    "\n",
    "# URL de la page web\n",
    "url = 'https://files.data.gouv.fr/geo-dvf/latest/csv/'\n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Utilisez BeautifulSoup pour analyser le HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Trouver toutes les balises <a>\n",
    "a_tags = soup.find_all('a')\n",
    "annees=[]\n",
    "# Parcourir chaque balise <a> et extraire le texte ainsi que la date/heure\n",
    "for a_tag in a_tags:\n",
    "    name = a_tag.text.strip()\n",
    "    if name!=\"../\":\n",
    "        annees.append(name)\n",
    "print(annees)\n",
    "\n",
    "url = f'https://files.data.gouv.fr/geo-dvf/latest/csv/{annee[-1]}'\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Trouver la balise 'a' qui contient les liens de téléchargement\n",
    "csv_element = soup.find('pre').find('a', {'href': \"full.csv.gz\"})\n",
    "csv_link = urljoin(url, csv_element['href'])\n",
    "\n",
    "# Téléchargement des datas et conversion en data frame\n",
    "csv_response = requests.get(csv_link)\n",
    "if csv_response.status_code == 200:\n",
    "    # Utilisation du buffer pour décompresser le fichier .gz\n",
    "    try :\n",
    "        with BytesIO(csv_response.content) as file_buffer:\n",
    "            with gzip.GzipFile(fileobj=file_buffer, mode='rb') as gz_file:\n",
    "                df = pd.read_csv(gz_file, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(\"Erreur dans la transformation des données en data frame :\",e)\n",
    "\n",
    "# Selection des données\n",
    "df = datas_processing.select_datas(df)\n",
    "\n",
    "# Gestion des valeurs manquantes :\n",
    "df = datas_processing.nan_management(df)\n",
    "\n",
    "# Adaptation des données du data frame au format de la base de données\n",
    "df = datas_processing.format_data(df)\n",
    "\n",
    "# Groupement des données sur une seule ligne par id_mutation (vente)\n",
    "df = datas_processing.grouped_datas(df)\n",
    "\n",
    "# Adaptation des données avec les variables et clés étrangères de la bdd\n",
    "df = datas_processing.features_and_foreign_keys(df,liste_ID_COMMUNES)\n",
    "\n",
    "#Suppression des lignes dupliquées\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "engine = connection_with_sqlalchemy(\"datagouv\")\n",
    "\n",
    "#//////////////////////////////////////////////////////////////////////////////\n",
    "#   Suppression de ventes identiques entre les nouvelles données récupérées\n",
    "#                    et celles stockées en base de données\n",
    "#//////////////////////////////////////////////////////////////////////////////\n",
    "transaction = engine.begin()\n",
    "try:\n",
    "    # Récupération la date la plus ancienne dans les nouvelles données récupérées\n",
    "    date_plus_ancienne = df['DATE_MUTATION'].min()\n",
    "    print(date_plus_ancienne)\n",
    "    # Requête pour supprimer les lignes avec une date supérieure ou égale à date_plus_ancienne\n",
    "    query_delete = f\"DELETE FROM VENTES WHERE DATE_MUTATION >= '{date_plus_ancienne}'\"\n",
    "    engine.execute(query_delete)\n",
    "\n",
    "    # Validez la transaction\n",
    "    transaction.commit()\n",
    "    print(\"Transaction validée avec succès.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # En cas d'erreur, annulez la transaction\n",
    "    print(f\"Erreur : {e}\")\n",
    "    transaction.rollback()\n",
    "    print(\"Transaction annulée.\")\n",
    "\n",
    "\n",
    "#//////////////////////////////////////////////////////////////////////////////\n",
    "#                   Insertion de nouvelles données\n",
    "#//////////////////////////////////////////////////////////////////////////////\n",
    "transaction = engine.begin()\n",
    "try:\n",
    "    # Insère les données dans la table VENTES\n",
    "    df.to_sql(name='VENTES', con=engine, if_exists='append', index=False)\n",
    "    \n",
    "    # Valide la transaction\n",
    "    transaction.commit()\n",
    "    print(\"Transaction validée avec succès.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # En cas d'erreur, annule la transaction\n",
    "    print(f\"Erreur : {e}\")\n",
    "    transaction.rollback()\n",
    "    print(\"Transaction annulée.\")\n",
    "\n",
    "finally:\n",
    "    # Libère les ressources de la connexion\n",
    "    transaction.close()\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération de la date de mise à jour du fichier\n",
    "date_element = soup.find('pre').contents[-1].strip()\n",
    "print(date_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"background-color:grey; color:white; padding: 5px;\">Entraînement d'un modèle<br></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se baser sur les modèles créé dans mlflow et entraîner un modèle sur les mêmes paramètres\n",
    "# Enregistrer dans mlflow, experiment : prod_{date_du_jour}\n",
    "gestion_outliers =True\n",
    "nb_ventes_mini = 10\n",
    "tx_filtrage =[nb_ventes_mini,30]\n",
    "region=''\n",
    "type_de_bien =''\n",
    "surface_terrain = True\n",
    "nb_mois=15\n",
    "param_grid = {\n",
    "    'n_estimators': [100,150],\n",
    "    'max_depth': [10],\n",
    "    'learning_rate': [0.1],\n",
    "    'min_child_weight': [1,3,5],\n",
    "    'alpha' : [0.5], \n",
    "    'lambda' : [0.5]\n",
    "}\n",
    "cv=5\n",
    "uri_tracking = \"https://mlflowimmoappkevleg-737621d410d0.herokuapp.com/\"\n",
    "experiment_name = \"En_Production\"\n",
    "\n",
    "# runs à effectuer all_datas 15months all_datas_surface_terrain 15months_surface_terrain)\n",
    "run_name = f\"{region}_{'Appartement_Maison' if len(type_de_bien)==0 else type_de_bien}\"\\\n",
    "     f\"_{nb_mois}months_GridSearch_GestionOutliers_{'oui' if gestion_outliers==True else 'non'}\"\\\n",
    "     f\"_{'' if surface_terrain==False else 'surface_terrain'}\"\n",
    "\n",
    "model_name = f\"Production_XGB_{run_name}\"\n",
    "\n",
    "where_clause, query = construcion_requete(region,\n",
    "                        type_de_bien,\n",
    "                        nb_ventes_mini,\n",
    "                        surface_terrain,\n",
    "                        nb_mois)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"background-color:grey; color:white; padding: 5px;\">Créer un csv pour être utilisé dans streamlit<br></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour éviter de trop requêter sur rds mais aussi avoir une execution rapide de l'application.\n",
    "# Récupérer les données Region, département, communes, prix_M²/mois sur la dernière année ou \n",
    "# les deux dernières années suivant le modèle utilisé et par type de bien\n",
    "df = loading_data(\"\"\"\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"background-color:grey; color:white; padding: 5px;\">Transformation du dataframe en csv et stockage dans AWS s3<br></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50 100 150 200 250 300 350]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.arange(50, 400, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print([i / 10.0 for i in range(1, 11)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
